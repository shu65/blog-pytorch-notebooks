{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch CUDA Graph",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMG9nkTpeawk3wvJYo8PHMy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shu65/blog-pytorch-notebooks/blob/main/pytorch_CUDA_Graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDm8-9ll7oqr",
        "outputId": "4f46c552-eeae-46fc-c8bc-e4e308570235"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 23 01:26:36 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y7vhThh7x41",
        "outputId": "a251f489-ef9d-421e-f7c7-562579dc8e02"
      },
      "source": [
        "!pip3 install torch==1.10.0+cu111 torchvision==0.11.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.10.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2137.6 MB)\n",
            "\u001b[K     |████████████▌                   | 834.1 MB 1.9 MB/s eta 0:11:21tcmalloc: large alloc 1147494400 bytes == 0x5560a3aa4000 @  0x7f2cbfa22615 0x556069f0f4cc 0x556069fef47a 0x556069f122ed 0x55606a003e1d 0x556069f85e99 0x556069f809ee 0x556069f13bda 0x556069f85d00 0x556069f809ee 0x556069f13bda 0x556069f82737 0x55606a004c66 0x556069f81daf 0x55606a004c66 0x556069f81daf 0x55606a004c66 0x556069f81daf 0x556069f14039 0x556069f57409 0x556069f12c52 0x556069f85c25 0x556069f809ee 0x556069f13bda 0x556069f82737 0x556069f809ee 0x556069f13bda 0x556069f81915 0x556069f13afa 0x556069f81c0d 0x556069f809ee\n",
            "\u001b[K     |███████████████▉                | 1055.7 MB 1.6 MB/s eta 0:11:19tcmalloc: large alloc 1434370048 bytes == 0x5560e80fa000 @  0x7f2cbfa22615 0x556069f0f4cc 0x556069fef47a 0x556069f122ed 0x55606a003e1d 0x556069f85e99 0x556069f809ee 0x556069f13bda 0x556069f85d00 0x556069f809ee 0x556069f13bda 0x556069f82737 0x55606a004c66 0x556069f81daf 0x55606a004c66 0x556069f81daf 0x55606a004c66 0x556069f81daf 0x556069f14039 0x556069f57409 0x556069f12c52 0x556069f85c25 0x556069f809ee 0x556069f13bda 0x556069f82737 0x556069f809ee 0x556069f13bda 0x556069f81915 0x556069f13afa 0x556069f81c0d 0x556069f809ee\n",
            "\u001b[K     |████████████████████            | 1336.2 MB 55.8 MB/s eta 0:00:15tcmalloc: large alloc 1792966656 bytes == 0x55606cf2c000 @  0x7f2cbfa22615 0x556069f0f4cc 0x556069fef47a 0x556069f122ed 0x55606a003e1d 0x556069f85e99 0x556069f809ee 0x556069f13bda 0x556069f85d00 0x556069f809ee 0x556069f13bda 0x556069f82737 0x55606a004c66 0x556069f81daf 0x55606a004c66 0x556069f81daf 0x55606a004c66 0x556069f81daf 0x556069f14039 0x556069f57409 0x556069f12c52 0x556069f85c25 0x556069f809ee 0x556069f13bda 0x556069f82737 0x556069f809ee 0x556069f13bda 0x556069f81915 0x556069f13afa 0x556069f81c0d 0x556069f809ee\n",
            "\u001b[K     |█████████████████████████▎      | 1691.1 MB 8.2 MB/s eta 0:00:55tcmalloc: large alloc 2241208320 bytes == 0x5560d7d14000 @  0x7f2cbfa22615 0x556069f0f4cc 0x556069fef47a 0x556069f122ed 0x55606a003e1d 0x556069f85e99 0x556069f809ee 0x556069f13bda 0x556069f85d00 0x556069f809ee 0x556069f13bda 0x556069f82737 0x55606a004c66 0x556069f81daf 0x55606a004c66 0x556069f81daf 0x55606a004c66 0x556069f81daf 0x556069f14039 0x556069f57409 0x556069f12c52 0x556069f85c25 0x556069f809ee 0x556069f13bda 0x556069f82737 0x556069f809ee 0x556069f13bda 0x556069f81915 0x556069f13afa 0x556069f81c0d 0x556069f809ee\n",
            "\u001b[K     |████████████████████████████████| 2137.6 MB 1.3 MB/s eta 0:00:01tcmalloc: large alloc 2137645056 bytes == 0x55615d676000 @  0x7f2cbfa211e7 0x556069f45067 0x556069f0f4cc 0x556069fef47a 0x556069f122ed 0x55606a003e1d 0x556069f85e99 0x556069f809ee 0x556069f13bda 0x556069f81c0d 0x556069f809ee 0x556069f13bda 0x556069f81c0d 0x556069f809ee 0x556069f13bda 0x556069f81c0d 0x556069f809ee 0x556069f13bda 0x556069f81c0d 0x556069f809ee 0x556069f13bda 0x556069f81c0d 0x556069f13afa 0x556069f81c0d 0x556069f809ee 0x556069f13bda 0x556069f82737 0x556069f809ee 0x556069f13bda 0x556069f82737 0x556069f809ee\n",
            "tcmalloc: large alloc 2672058368 bytes == 0x5561dcd14000 @  0x7f2cbfa22615 0x556069f0f4cc 0x556069fef47a 0x556069f122ed 0x55606a003e1d 0x556069f85e99 0x556069f809ee 0x556069f13bda 0x556069f81c0d 0x556069f809ee 0x556069f13bda 0x556069f81c0d 0x556069f809ee 0x556069f13bda 0x556069f81c0d 0x556069f809ee 0x556069f13bda 0x556069f81c0d 0x556069f809ee 0x556069f13bda 0x556069f81c0d 0x556069f13afa 0x556069f81c0d 0x556069f809ee 0x556069f13bda 0x556069f82737 0x556069f809ee 0x556069f13bda 0x556069f82737 0x556069f809ee 0x556069f14271\n",
            "\u001b[K     |████████████████████████████████| 2137.6 MB 173 bytes/s \n",
            "\u001b[?25hCollecting torchvision==0.11.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.11.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (21.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.9 MB 58 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0+cu111) (3.7.4.3)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.1+cu111) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.1+cu111) (1.19.5)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu111\n",
            "    Uninstalling torch-1.9.0+cu111:\n",
            "      Successfully uninstalled torch-1.9.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu111\n",
            "    Uninstalling torchvision-0.10.0+cu111:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.10.0+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.0+cu111 torchvision-0.11.1+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4dI29n-weXJ",
        "outputId": "8c3fb75f-d73a-491d-af92-d93384e710fd"
      },
      "source": [
        "!pip list | grep torch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch                         1.10.0+cu111\n",
            "torchsummary                  1.5.1\n",
            "torchtext                     0.10.0\n",
            "torchvision                   0.11.1+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LtwUl32wnn6",
        "outputId": "f7cbe4f6-b651-4330-bcba-d29c09db740f"
      },
      "source": [
        "import torch\n",
        "\n",
        "static_input = torch.empty((5,), device=\"cuda\")\n",
        "# Warmup before capture\n",
        "s = torch.cuda.Stream()\n",
        "s.wait_stream(torch.cuda.current_stream())\n",
        "with torch.cuda.stream(s):\n",
        "    for _ in range(3):\n",
        "        static_output = static_input * 2\n",
        "torch.cuda.current_stream().wait_stream(s)\n",
        "\n",
        "# Captures the graph\n",
        "g = torch.cuda.CUDAGraph()\n",
        "with torch.cuda.graph(g):\n",
        "    static_output = static_input * 2\n",
        "\n",
        "# Fills the graph's input memory with new data to compute on\n",
        "static_input.copy_(torch.full((5,), 3, device=\"cuda\"))\n",
        "print(\"input of cuda graph\", static_input)\n",
        "g.replay()\n",
        "# static_output holds the results\n",
        "print(\"output of cuda graph\", static_output)  # full of 3 * 2 = 6\n",
        "\n",
        "# Fills the graph's input memory with more data to compute on\n",
        "static_input.copy_(torch.full((5,), 4, device=\"cuda\"))\n",
        "print(\"input of cuda graph\", static_input)\n",
        "g.replay()\n",
        "print(\"output of cuda graph\",static_output)  # full of 4 * 2 = 8"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input of cuda graph tensor([3., 3., 3., 3., 3.], device='cuda:0')\n",
            "output of cuda graph tensor([6., 6., 6., 6., 6.], device='cuda:0')\n",
            "input of cuda graph tensor([4., 4., 4., 4., 4.], device='cuda:0')\n",
            "output of cuda graph tensor([8., 8., 8., 8., 8.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoJ8yRACxL78",
        "outputId": "9fda257b-df93-4295-fc28-11d82de4d497"
      },
      "source": [
        "import time\n",
        "\n",
        "import torch\n",
        "\n",
        "def training_step(model, loss_fn, optimizer, data, target):\n",
        "    y_pred = model(data)\n",
        "    loss = loss_fn(y_pred, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "N, D_in, H, D_out = 32, 128, 256, 16\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.Dropout(p=0.2),\n",
        "    torch.nn.Linear(H, D_out),\n",
        "    torch.nn.Dropout(p=0.1)\n",
        ").cuda()\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Placeholders used for capture\n",
        "static_input = torch.randn(N, D_in, device='cuda')\n",
        "static_target = torch.randn(N, D_out, device='cuda')\n",
        "\n",
        "# warmup\n",
        "torch.cuda.synchronize()\n",
        "for _ in range(3):\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    training_step(model, loss_fn, optimizer, static_input, static_target)\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "# capture\n",
        "g = torch.cuda.CUDAGraph()\n",
        "optimizer.zero_grad(set_to_none=True)\n",
        "with torch.cuda.graph(g):\n",
        "    training_step(model, loss_fn, optimizer, static_input, static_target)\n",
        "\n",
        "n_trials = 10\n",
        "real_inputs = [torch.rand_like(static_input) for _ in range(n_trials)]\n",
        "real_targets = [torch.rand_like(static_target) for _ in range(n_trials)]\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "for data, target in zip(real_inputs, real_targets):\n",
        "    # Fills the graph's input memory with new data to compute on\n",
        "    static_input.copy_(data)\n",
        "    static_target.copy_(target)\n",
        "    training_step(model, loss_fn, optimizer, static_input, static_target)\n",
        "torch.cuda.synchronize()\n",
        "elapsed_time = time.time() - start\n",
        "print(\"avg cuda default:\", elapsed_time/n_trials, 'sec.')\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "for data, target in zip(real_inputs, real_targets):\n",
        "    # Fills the graph's input memory with new data to compute on\n",
        "    static_input.copy_(data)\n",
        "    static_target.copy_(target)\n",
        "    g.replay()\n",
        "torch.cuda.synchronize()\n",
        "elapsed_time = time.time() - start\n",
        "\n",
        "print(\"avg cuda graph:\", elapsed_time/n_trials, 'sec.')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg cuda default: 0.0011144161224365234 sec.\n",
            "avg cuda graph: 0.00047113895416259763 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47IRX0Eex2XS",
        "outputId": "05028ab4-98f5-4e22-fca9-ef84ed34ef9b"
      },
      "source": [
        "import time\n",
        "\n",
        "import torch\n",
        "\n",
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))\n",
        "\n",
        "n_trials = 10000\n",
        "input_batch_cpu = torch.randn(1, 3, 224, 224)\n",
        "#input_batch_cpu = torch.randn(32, 3, 224, 224)\n",
        "\n",
        "input_batch_gpu = input_batch_cpu.clone().detach().to('cuda')\n",
        "\n",
        "# default\n",
        "out = gelu(input_batch_gpu)\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "for i in range(n_trials):\n",
        "    out = gelu(input_batch_gpu)\n",
        "torch.cuda.synchronize()\n",
        "elapsed_time = time.time() - start\n",
        "print(\"avg default:\", elapsed_time/n_trials, 'sec.')\n",
        "\n",
        "# torch.jit.script\n",
        "torch.jit._state._jit_function_overload_caching.clear()\n",
        "torch.jit._state._jit_caching_layer.clear()\n",
        "\n",
        "scripted_gelu = torch.jit.script(gelu)\n",
        "out = scripted_gelu(input_batch_gpu)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "for i in range(n_trials):\n",
        "    out = scripted_gelu(input_batch_gpu)\n",
        "torch.cuda.synchronize()\n",
        "elapsed_time = time.time() - start\n",
        "print(\"avg torch.jit.script:\", elapsed_time/n_trials, 'sec.')\n",
        "\n",
        "# CUDA Graph\n",
        "gelu_graph = torch.cuda.CUDAGraph()\n",
        "static_input = torch.empty_like(input_batch_gpu)\n",
        "# Warmup before capture\n",
        "torch.cuda.synchronize()\n",
        "static_output = gelu(static_input)\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "with torch.cuda.graph(gelu_graph):\n",
        "    static_output = gelu(static_input)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "for i in range(n_trials):\n",
        "    gelu_graph.replay()\n",
        "torch.cuda.synchronize()\n",
        "elapsed_time = time.time() - start\n",
        "print(\"avg cuda graph:\", elapsed_time/n_trials, 'sec.')\n",
        "\n",
        "scripted_gelu_graph = torch.cuda.CUDAGraph()\n",
        "static_input = torch.empty_like(input_batch_gpu)\n",
        "# Warmup before capture\n",
        "torch.cuda.synchronize()\n",
        "static_output = scripted_gelu(static_input)\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "with torch.cuda.graph(scripted_gelu_graph):\n",
        "    static_output = scripted_gelu(static_input)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "for i in range(n_trials):\n",
        "    scripted_gelu_graph.replay()\n",
        "torch.cuda.synchronize()\n",
        "elapsed_time = time.time() - start\n",
        "print(\"avg torch.jit.script and cuda graph:\", elapsed_time/n_trials, 'sec.')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg default: 7.09254503250122e-05 sec.\n",
            "avg torch.jit.script: 3.8933825492858885e-05 sec.\n",
            "avg cuda graph: 6.485035419464111e-05 sec.\n",
            "avg torch.jit.script and cuda graph: 3.563470840454101e-05 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzEcLLjkyi6k",
        "outputId": "538c710d-81da-4534-dbb2-001ee6d7cbb4"
      },
      "source": [
        "import time\n",
        "\n",
        "import torch\n",
        "\n",
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))\n",
        "\n",
        "n_trials = 10000\n",
        "#input_batch_cpu = torch.randn(1, 3, 224, 224)\n",
        "input_batch_cpu = torch.randn(32, 3, 224, 224)\n",
        "\n",
        "input_batch_gpu = input_batch_cpu.clone().detach().to('cuda')\n",
        "\n",
        "# default\n",
        "out = gelu(input_batch_gpu)\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "for i in range(n_trials):\n",
        "    out = gelu(input_batch_gpu)\n",
        "torch.cuda.synchronize()\n",
        "elapsed_time = time.time() - start\n",
        "print(\"avg default:\", elapsed_time/n_trials, 'sec.')\n",
        "\n",
        "# torch.jit.script\n",
        "torch.jit._state._jit_function_overload_caching.clear()\n",
        "torch.jit._state._jit_caching_layer.clear()\n",
        "\n",
        "scripted_gelu = torch.jit.script(gelu)\n",
        "out = scripted_gelu(input_batch_gpu)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "for i in range(n_trials):\n",
        "    out = scripted_gelu(input_batch_gpu)\n",
        "torch.cuda.synchronize()\n",
        "elapsed_time = time.time() - start\n",
        "print(\"avg torch.jit.script:\", elapsed_time/n_trials, 'sec.')\n",
        "\n",
        "# CUDA Graph\n",
        "gelu_graph = torch.cuda.CUDAGraph()\n",
        "static_input = torch.empty_like(input_batch_gpu)\n",
        "# Warmup before capture\n",
        "torch.cuda.synchronize()\n",
        "static_output = gelu(static_input)\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "with torch.cuda.graph(gelu_graph):\n",
        "    static_output = gelu(static_input)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "for i in range(n_trials):\n",
        "    gelu_graph.replay()\n",
        "torch.cuda.synchronize()\n",
        "elapsed_time = time.time() - start\n",
        "print(\"avg cuda graph:\", elapsed_time/n_trials, 'sec.')\n",
        "\n",
        "scripted_gelu_graph = torch.cuda.CUDAGraph()\n",
        "static_input = torch.empty_like(input_batch_gpu)\n",
        "# Warmup before capture\n",
        "torch.cuda.synchronize()\n",
        "static_output = scripted_gelu(static_input)\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "with torch.cuda.graph(scripted_gelu_graph):\n",
        "    static_output = scripted_gelu(static_input)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "for i in range(n_trials):\n",
        "    scripted_gelu_graph.replay()\n",
        "torch.cuda.synchronize()\n",
        "elapsed_time = time.time() - start\n",
        "print(\"avg torch.jit.script and cuda graph:\", elapsed_time/n_trials, 'sec.')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "avg default: 0.001322664523124695 sec.\n",
            "avg torch.jit.script: 0.00042458438873291016 sec.\n",
            "avg cuda graph: 0.0013399296522140502 sec.\n",
            "avg torch.jit.script and cuda graph: 0.00037378573417663573 sec.\n"
          ]
        }
      ]
    }
  ]
}
